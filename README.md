# Programmers 데브코스 Data Engineering 5기 최종 프로젝트
## 주제: 사람인 API를 활용한 개인화 대시보드 및 채용공고 추천시스템
- <b>Team</b> `4람in`
  - <b>전병운 `팀장 / EC2, Airflow 서버 구축 및 Glue scheduler 작업, 총괄관리`</b>
  - <b>김동환 `데이터 전처리, Preset 대시보드 작업, Airflow Dag 작성`</b>
  - <b>설연수 `데이터 추출 및 적재, 데이터 관리, Preset 대시보드 작업`</b>
  - <b>박지연 `데이터 마트 구축, Athena 활용 및 데이터 분석용 테이블 작성`</b>
- **프로젝트 진행 기간**: 2025.02.26 ~ 2025.03.18

## 목차
###### 1. 프로젝트 개요
###### 2. 프로젝트 주제 선정 이유
###### 3. 프로젝트 세부 결과
###### 4. 대시보드
###### 5. 활용 기술 및 프레임워크
###### 6. 프로젝트 회고

## 1. 프로젝트 개요
### 1-1 문제 정의
기존 채용 사이트에서는 구직자가 원하는 공고를 찾기 위해 **직접 검색하고 필터링하는 과정이 필요**합니다.

이 과정에서 **정보가 분산되고 실시간으로 원하는 공고를 놓칠 가능성이 크며**, 특히 신입 및 경력 구직자들에게는 **최신 트렌드와 채용 시장의 변화를 파악하는 것이 어렵습니다**.

또한, 기존 추천 시스템들은 단순한 키워드 매칭에 의존하는 경우가 많아, **개인의 관심사와 실제 시장 트렌드를 반영하지 못하는 한계**가 있습니다.

### 1-2 목표
본 프로젝트는 **구직자가 원하는 직무를 설정하면, 해당 직무와 연관된 채용 공고를 자동으로 수집·분석하고, 맞춤형 공고를 추천하는 시스템을 구축**하는 것을 목표로 합니다.

이를 통해 사용자는 **검색 없이도 최신 공고 및 마감 임박 공고를 실시간으로 확인**할 수 있으며, **채용 시장의 변화와 트렌드까지 한눈에 파악할 수 있도록 지원**합니다.

### 1-3 해결 방법
- **채용 공고 데이터 수집**
- **데이터 처리 및 분석**
- **개인화된 공고 추천 시스템 개발**
- **대시보드 시각화**

### 1-4 프로젝트 아키텍처
<img src="https://github.com/user-attachments/assets/9996188f-3554-46ae-bb45-645173d3f3b6"  width="1000" height="500"/><br>

본 프로젝트의 아키텍처는 **데이터 수집 → 데이터 처리 → 데이터 저장 및 분석 → 시각화**의 4단계로 구성되며 **개인화된 채용 공고 추천 시스템**을 구축하는 것을 목표로 합니다.

기존 채용 사이트에서는 **구직자가 직접 검색해야 하는 불편함**이 존재하며, 이를 해결하기 위해 **완전 자동화된 데이터 파이프라인을 설계**하였습니다.

특히, 본 프로젝트는 **DEV 데이터 엔지니어링 과정에서 배운 핵심 기술(AWS Glue, Spark, Airflow 등)을 적극 활용**하고, 비용 효율성과 성능을 모두 고려한 설계이며,

실제 **실무에서 사용되는 아키텍처와 유사한 구조**를 구현하고자 하였습니다.

### 1-5 기대 효과
- **구직자의 공고 탐색 부담을 줄이고, 맞춤형 공고 추천으로 채용 기회를 극대화**
- **실시간 채용 트렌드를 제공하여 구직자가 시장 변화를 빠르게 인지할 수 있도록 지원**
- **AWS Glue와 Spark 기반의 비용 효율적인 데이터 처리 환경을 구축하여 운영 비용 절감**
- **자동화된 데이터 분석 및 추천 시스템을 통해 인사이트 제공 및 채용 공고 접근성 향상**

## 2. 프로젝트 주제 선정 이유
### 2-1 기존 채용 사이트의 한계를 경험한 구직자의 관점
우리 팀원들은 모두 **구직을 준비 중이며, 사람인과 같은 채용 사이트를 적극적으로 활용**하고 있습니다.

하지만, 기존 채용 사이트를 이용하는 과정에서 여러 **비효율적인 문제점**을 경험하였습니다.
- **반복적인 검색 과정**
- **개인화 부족**
- **비효율적인 UI/UX**

### 2-2 개인화된 채용 공고 대시보드의 필요성
이러한 문제를 해결하기 위해, 우리는 개인 맞춤형 채용 공고를 자동으로 제공하는 시스템을 기획하였습니다.
- **사용자가 원하는 직무(예: 데이터 엔지니어)를 설정하면, 최신 공고와 마감 임박 공고를 자동 추천**
- **사용자의 지원 가능성을 고려한 공고 필터링 및 맞춤 추천 기능 제공**
- **채용 시장 트렌드를 시각화하여 한눈에 산업 동향을 파악할 수 있도록 대시보드 구성**
이를 통해, 검색 없이도 구직자가 원하는 정보를 빠르게 얻고, 보다 효율적으로 채용 공고를 탐색할 수 있도록 지원하는 것이 본 프로젝트의 핵심 목표입니다.

## 3. 프로젝트 세부 결과
### 3-1 인프라 구축
이번 프로젝트에서는 **AWS 기반의 데이터 파이프라인을 구축하여, 서버리스 환경에서 채용 공고 데이터를 수집, 처리, 저장 및 분석**하는 시스템을 설계하였습니다.

특히, **EC2, S3, Glue, Athena를 활용하여 비용 효율적인 데이터 처리 인프라**를 구축하였습니다.

#### 1) EC2 인스턴스 구축
#### 2) S3 버킷 구성 및 활용
#### 3) AWS Glue - ETL Job 및 Crawler 활용
#### 4) AWS Athena - 데이터 분석 및 대시보드 연동

### 3-2 데이터 레이크
#### 1) 원본 데이터 수집
- **API 호출 정책 및 데이터 수집 범위**
  - 사용 API: 사람인 채용공고 API 
  - 데이터 요청 주기: 오전 10시 ~ 오후 8시, 2시간 간격으로 하루 총 6회 호출 
  - 데이터 수집 기준: IT개발·데이터 업종과 직무 공고만 필터링 
  - 호출 파라미터 :
    
  <img src="https://github.com/user-attachments/assets/5093b9c2-053a-42f1-8a6a-38cd84197b3d"  width="400" height="120"/><br>
- **데이터 저장**
  
  <img src="https://github.com/user-attachments/assets/48539a08-9a62-4c7c-9120-cf30f731f486"  width="400" height="120"/><br>

#### 2) 데이터 전처리 및 Parquet 변환
- **AWS Glue**
  - Glue에서 SparSession 생성하여 Spark로 분산처리
  - S3의 버킷에 저장 후 Glue에서 Script 호출
- **데이터 저장**
  
  <img src="https://github.com/user-attachments/assets/37f24811-5647-44a0-b9ab-c1e2bf48e9cc"  width="400" height="120"/><br>
- **Parquet 파일 구조**
  - 데이터 분석에 필요하지 않은 정보 제거 (필터링)
  - unixtime 형식을 datetime 형식으로 변경

#### 3) Airflow DAG
- **원본 데이터 수집 task**
  - Airflow에서 실행
  - 사람인 API 호출하여 데이터 수집 후 JSON으로 S3 버킷에 저장
- **전처리 및 Parquet변환 task**
  - Airflow에서 Glue Job 생성 및 호출
  - Glue에서 Spark사용하여 전처리 후 Parquet로 변환하여 S3 버킷에 저장

### 3-3 데이터 마트
#### 1) 원본 데이터를 분석용으로 적합하게 만들기 위한 작업 수행
- **날짜/시간 변환**
- **컬럼 정리**
- **데이터 분해**
- **지역 정보 정제**
- **분석용 테이블 저장**

#### 2) Airflow DAG를 통한 Glue Job 실행
- **'--s3_folder': "{{ execution_date.strftime('s3://saramin-data-bucket/saramin/process_data/%Y/%m/%d/') }}"** 이 인자가 Glue 스크립트에 전달
- **이를 통해 각 Glue Job 실행 시점에 맞는 데이터를 읽어 들여 날짜별 데이터 관리가 자동화**

#### 3) Airflow DAG
- DAG가 스케줄에 따라 트리거되면 task 실행
- 동적 날짜 폴더 인자에 의해 각 실행마다 해당 실행 일자에 해당하는 S3 폴더를 읽어 최신 데이터를 처리할 수 있다.

### 3-4 머신러닝 기반 채용 공고 추천 실험 (Korean BERT)
본 프로젝트에서 머신러닝 기반 추천 시스템을 완전히 구축하지는 못했지만, **Korean BERT를 활용한 채용 공고 추천 가능성을 실험**해 보았습니다.

기존의 단순 필터링 방식에서 벗어나, **사용자 입력값을 "스킬셋"이라고 가정하고 채용 공고 데이터에서 `job_name` 필드를 읽어 유사도를 비교하는 방식**을 적용했습니다.

#### 1) 실험 방법
- **사용자 입력값**: 특정 기술 스택(예: "Python, Spark, Airflow")을 입력값으로 설정
- **채용 공고 데이터**: 사람인 API에서 가져온 데이터 중 `job_name` 필드를 활용
- **모델 적용**:
    - **Korean BERT 임베딩 활용**하여 **사용자 입력과 채용 공고의 `job_name` 필드를 벡터화**
    - 벡터 간 코사인 유사도를 계산하여 **사용자 입력과 가장 유사한 채용 공고 추천**

#### 2) 실험 결과
- **단순 키워드 매칭보다 더 정교한 추천 가능성 확인**
- 특정 기술 스택을 입력했을 때, 기존 필터링 방식보다 **직무명(job_name)과의 연관성이 높은 채용 공고가 추천됨**
- 하지만, **공고의 상세 내용을 반영하지 못해 한계가 있었음**

#### 3) 개선 방향
- 현재는 `job_name` 필드만 사용했지만, **채용 공고의 상세 내용을 함께 분석하여 더 정교한 추천 모델 구축 필요**
- Korean BERT 외에도, **Word2Vec 또는 TF-IDF 기반 추천 모델을 비교 실험하여 최적의 방법 찾기**
- 사용자의 **이력서 또는 상세 기술 스택을 반영한 맞춤형 추천 시스템 구축**

## 4. 대시보드



## 5. 활용 기술 및 프레임워크
### 데이터 수집
`Apache Airflow` `사람인 API`

### 데이터 저장
`AWS S3`

### 데이터 변환
`AWS Glue, Apache Spark`

### 자동화 및 환경 구축
`AWS EC2`

### 시각화
`Preset io`

## 6. 프로젝트 회고
### 6-1 프로젝트 결과물에 대한 완성도 평가 (10점 만점 기준)
<img src="https://github.com/user-attachments/assets/cb7dc1a4-8e13-4766-94e3-70cbb851dd7b"  width="1000" height="400"/><br>
- **종합 점수: 7.7/10**
- **기본적인 데이터 엔지니어링 파이프라인은 완성도 높게 구축했지만, 개인화된 추천 기능을 강화하면 더욱 효과적인 시스템이 될 것**

### 6-2 잘한 점과 아쉬운 점
#### 잘한 점
- **AWS Glue + Spark를 활용한 서버리스 아키텍처 적용**
- **개인화된 대시보드를 통해 맞춤형 공고 추적 가능**

#### 아쉬운 점
- **사용자 입력 방식의 한계**
- **머신러닝 기반 추천 시스템 미적용**

### 6-3 추후 개선점 및 보완할 점
1) **사용자 입력 방식 개선**
2) **머신러닝 기반 공고 추천 기능 추가**
3) **사람인 API 활용 방식 개선**

### 6-4 프로젝트 수행을 통해 경험한 성과
- **데이터 엔지니어로서 실무에 가까운 프로젝트 경험**
- **실제 데이터 활용 프로젝트에서의 인사이트**
- **데이터 시각화 경험**
